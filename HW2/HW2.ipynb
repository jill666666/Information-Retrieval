{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit9e766e2947134364a563550b140f8675",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Parse, Stem, and Tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "import pprint\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "docs_dict = {}\n",
    "unqno = 1\n",
    "unq_dict = {}\n",
    "\n",
    "\n",
    "stoplist = open('../HW1/reference/stoplist.txt')\n",
    "\n",
    "stop_arr = []\n",
    "for line in stoplist:\n",
    "    stop_arr.append(line.strip())\n",
    "\n",
    "def parse_docs(docs):\n",
    "    while '<DOC>' in docs:\n",
    "        text = \"\"\n",
    "        docend = docs.find('</DOC>')\n",
    "        substr = docs[:docend]\n",
    "        d_stt = substr.find('<DOCNO>') + len('<DOCNO>')\n",
    "        d_end = substr.find('</DOCNO>')\n",
    "        docno = substr[d_stt:d_end].strip()\n",
    "        while \"<TEXT>\" in substr:\n",
    "            t_stt = substr.find('<TEXT>') + len('<TEXT>')\n",
    "            t_end = substr.find('</TEXT>')\n",
    "            text = text + substr[t_stt:t_end].strip() + '\\n'\n",
    "            substr = substr[t_end + len('</TEXT>'):]\n",
    "        docs = docs[docend + len('</DOC>'):]\n",
    "        docs_dict[docno] = text\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def stemmed_tokens(docno, text):\n",
    "    global unqno\n",
    "    count = 1\n",
    "    token_arr = []\n",
    "    temp_arr = []\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token != \"'s\"]\n",
    "    stemmed_tokens = [element for item in stemmed_tokens for element in item.split('-')]\n",
    "    for i in stemmed_tokens:\n",
    "        i = i.lower()\n",
    "        if i not in stop_arr and re.search('[a-zA-Z0-9]', str(i)):\n",
    "            if i not in unq_dict.keys():\n",
    "                arr = [i, unqno, docno, count]\n",
    "                unq_dict[i] = unqno\n",
    "                unqno += 1\n",
    "                count += 1\n",
    "            else:\n",
    "                arr = [i, unq_dict[i], docno, count]\n",
    "                count += 1\n",
    "            token_arr.append(arr)\n",
    "    return token_arr\n",
    "\n",
    "def unstemmed_tokens(docno, text):\n",
    "    global unqno\n",
    "    count = 1\n",
    "    token_arr = []\n",
    "    temp_arr = []\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in tokens:\n",
    "        i = i.lower()\n",
    "        if i not in stop_arr and re.search('[a-zA-Z0-9]', str(i)):\n",
    "            if i not in unq_dict.keys():\n",
    "                arr = [i, unqno, docno, count]\n",
    "                unq_dict[i] = unqno\n",
    "                unqno += 1\n",
    "                count += 1\n",
    "            else:\n",
    "                arr = [i, unq_dict[i], docno, count]\n",
    "                count += 1\n",
    "            token_arr.append(arr)\n",
    "    return token_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 365/365 [00:03<00:00, 101.60it/s]\n"
    },
    {
     "data": {
      "text/plain": "84678"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bulk_index(filepath):\n",
    "    for file in tqdm(os.listdir(filepath), position=0, leave=True):\n",
    "        parse_docs(read_file(os.path.join(filepath, file)))\n",
    "\n",
    "bulk_index('../AP_DATA/ap89_collection')\n",
    "\n",
    "len(docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_dict():\n",
    "    stem_dict = {}\n",
    "    global unq_dict\n",
    "    for key, words in tqdm(docs_dict.items(), position=0, leave=True, desc='Stemming and tokenizing...'):\n",
    "        stem_dict[key] = stemmed_tokens(key, words)\n",
    "    return stem_dict\n",
    "\n",
    "def unstemmed_dict():\n",
    "    unstem_dict = {}\n",
    "    global unq_dict\n",
    "    for key, words in tqdm(docs_dict.items(), position=0, leave=True, desc='Stemming and tokenizing...'):\n",
    "        unstem_dict[key] = unstemmed_tokens(key, words)\n",
    "    return unstem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Stemming and tokenizing...: 100%|██████████| 84678/84678 [22:15<00:00, 63.39it/s]\n"
    }
   ],
   "source": [
    "stemmed_dict = stemmed_dict()\n",
    "# unstemmed_dict = unstemmed_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inverted Index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create partial lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(stem_dict):\n",
    "    offset = 0\n",
    "    count = 1\n",
    "    temp_dict = {}\n",
    "    for docno, tuples in tqdm(stem_dict.items(), position=0, leave=True, desc='Creating inverted indexes...'):\n",
    "        position_dict = {}\n",
    "        for tpl in tuples:\n",
    "            if tpl[0] not in position_dict.keys():\n",
    "                position_dict[tpl[0]] = [n[3] for n in tuples if n[0] == tpl[0]]\n",
    "        for tpl in tuples:\n",
    "            if tpl[0] not in temp_dict.keys():\n",
    "                temp_dict[tpl[0]] = [[docno, len(position_dict[tpl[0]]), position_dict[tpl[0]]]]\n",
    "            elif tpl[2] not in [n[0] for n in temp_dict[tpl[0]]]:\n",
    "                temp_dict[tpl[0]].append([docno, len(position_dict[tpl[0]]), position_dict[tpl[0]]])\n",
    "        offset += 1\n",
    "        if offset == 1000:\n",
    "            filename = \"./batches/batch\"+str(count)+\".txt\"\n",
    "            count += 1\n",
    "            f = open(filename, 'w', encoding=\"ISO-8859-1\")\n",
    "            f.write(json.dumps(temp_dict, separators=(',', ':')))\n",
    "            temp_dict = {}\n",
    "            offset = 0\n",
    "    filename = \"./batches/batch\"+str(count)+\".txt\"\n",
    "    f = open(filename, 'w', encoding=\"ISO-8859-1\")\n",
    "    f.write(json.dumps(temp_dict, separators=(',', ':')))\n",
    "    temp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Creating inverted indexes...: 100%|██████████| 84678/84678 [09:44<00:00, 144.79it/s]\n"
    }
   ],
   "source": [
    "inverted_index(stemmed_dict)\n",
    "# inverted_index(unstemmed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Merge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge using Offset and Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_merge():\n",
    "    catalog_counter = 1\n",
    "    str1 = \"./batches/batch\"+ str(catalog_counter) + \".txt\"\n",
    "\n",
    "    f = open(str1, 'a')\n",
    "    offset = 0\n",
    "    i = 0\n",
    "    count = 0\n",
    "    dict_catalog = {}\n",
    "    for k in tqdm(stemmed_dict, position=0, leave=True):\n",
    "        if (i == 1):\n",
    "            offset = offset + len(k) + len(\" \")\n",
    "            size = len(str(stemmed_dict[k])) + len(\"\\n\")\n",
    "            str_catalog = str(k) + \" \" + str(offset) + \" \" + str(size) + \"\\n\"\n",
    "            dict_catalog[k] = [offset, size]\n",
    "            offset = offset + size\n",
    "        else:\n",
    "            offset = len(k) + len(\" \")\n",
    "            size = len(str(stemmed_dict[k])) + len(\"\\n\")\n",
    "            str_catalog = str(k) + \" \" + str(offset) + \" \" + str(size) + \"\\n\"\n",
    "            dict_catalog[k] = [offset, size]\n",
    "            offset = offset + size\n",
    "            i = 1\n",
    "        g.write(str(k) + \" \" + str(stemmed_dict[k]) + \"\\n\")\n",
    "        f.write(str_catalog)\n",
    "        count+=1\n",
    "        if count == 1000:\n",
    "            catalog_counter += 1\n",
    "            str1 = \"./Data/Catalog/\"+\"catalog\"+ str(catalog_counter) + \".txt\"\n",
    "            filename = \"./Data/Index/\"+\"inverted_index\" + str(catalog_counter) + \".txt\"\n",
    "            f = open(str1, 'a')\n",
    "            g = open(filename, 'a')\n",
    "            count = 0\n",
    "    g.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_catalog(catalog_counter):\n",
    "    index_str = \"./indices/inverted_index\" + str(catalog_counter) + \".txt\"\n",
    "    batch_str = \"./batches/batch\" + str(catalog_counter) + \".txt\"\n",
    "    catalog_str = \"./catalogs/catalog\" + str(catalog_counter) + \".txt\"\n",
    "    with open(batch_str) as file:\n",
    "        index_dict = json.loads(file.read())\n",
    "    catalog_f = open(catalog_str, 'a')\n",
    "    index_f = open(index_str, 'a')\n",
    "    offset = 0\n",
    "    i = 0\n",
    "    count = 0\n",
    "    catalog_dict = {}\n",
    "    for key in index_dict:\n",
    "        offset = offset + len(key) + len(\" \")\n",
    "        size = len(str(index_dict[key])) + len(\"\\n\")\n",
    "        str_catalog = str(key) + \" \" + str(offset) + \" \" + str(size) + \"\\n\"\n",
    "        catalog_dict[key] = [offset, size]\n",
    "        offset = offset + size\n",
    "        index_f.write(str(key) + \" \" + str(index_dict[key]) + \"\\n\")\n",
    "        catalog_f.write(str_catalog)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_merge()\n",
    "for i in range(85):\n",
    "    create_catalog(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def merge_using_catalogs(counter, merged_dict):\n",
    "    temp_dict={}\n",
    "    catalog_str = \"./catalogs/catalog\" + str(counter) + \".txt\"\n",
    "    index_str = \"./indices/inverted_index\" + str(counter) + \".txt\"\n",
    "    catalog_dict = {}\n",
    "    iList1 = open(index_str, 'r')\n",
    "    with open(catalog_str) as fh:\n",
    "        for line in fh:\n",
    "            word, description = line.strip().split(' ', 1)\n",
    "            catalog_dict[word] = description.split()\n",
    "    for i in catalog_dict.keys():\n",
    "        iList1.seek(0)\n",
    "        list_offset = catalog_dict[i]\n",
    "        o = int(list_offset[0])\n",
    "        s = list_offset[1]\n",
    "        s = int(s)\n",
    "        iList1.seek(o)\n",
    "        data1 = iList1.read(s)\n",
    "        data1 = data1.rstrip()\n",
    "        data1 = ast.literal_eval(data1)\n",
    "        temp_dict[i]=data1\n",
    "    for key in temp_dict.keys():\n",
    "        if key in merged_dict:\n",
    "            for i in range(len(temp_dict[key])):\n",
    "                merged_dict[key].append(temp_dict[key][i])\n",
    "        else:\n",
    "            merged_dict[key] = temp_dict[key]\n",
    "\n",
    "def single_inverted_index(num_files):\n",
    "    merged_dict = {}\n",
    "    for i in tqdm(range(num_files)):\n",
    "        merge_using_catalogs(i+1, merged_dict)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "   merged_dict = {**dict1, **dict2}\n",
    "   for key, value in merged_dict.items():\n",
    "       if key in dict1 and key in dict2:\n",
    "            for i in range(len(dict1[key])):\n",
    "                merged_dict[key].append(dict1[key][i])\n",
    "   return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 85/85 [03:57<00:00,  2.80s/it]\n"
    }
   ],
   "source": [
    "merged_dict = single_inverted_index(85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge using **kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 86/86 [02:47<00:00,  1.95s/it]\n"
    },
    {
     "data": {
      "text/plain": "453995399"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged={}\n",
    "for file in tqdm(os.listdir('./batches'), position=0, leave=True):\n",
    "    if file != '.DS_Store':\n",
    "        with open(os.path.join('./batches', file), encoding=\"ISO-8859-1\") as file:\n",
    "            dict2 = json.load(file)\n",
    "            merged = merge_dict(merged, dict2)\n",
    "\n",
    "f = open('merged.txt', 'w', encoding=\"ISO-8859-1\")\n",
    "f.write(json.dumps(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"merged.txt\") as file:\n",
    "    data = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Compress and Decompress]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Using GZip Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "fp = open(\"merged.txt\",\"rb\")\n",
    "data = fp.read()\n",
    "bindata = bytearray(data)\n",
    "with gzip.open(\"merged.txt.gz\", \"wb\") as f:\n",
    "    f.write(bindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with gzip.open(\"merged.txt.gz\", \"rb\") as f:\n",
    "\tdata = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Uncompressed Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docno_map = {}\n",
    "word_map = {}\n",
    "docno_counter = 0\n",
    "word_counter = 0\n",
    "for docno in docs_dict:\n",
    "    docno_map[docno] = docno_counter\n",
    "    docno_counter += 1\n",
    "for word in merged_dict:\n",
    "    word_map[word] = word_counter\n",
    "    word_counter += 1\n",
    "for key in merged_dict:\n",
    "    for item in merged_dict[key]:\n",
    "        item[0] = docno_map[item[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_key_name(merged_dict):\n",
    "    for key in tqdm(merged_dict, desc='Changing key name'):\n",
    "        if str(key) in word_map.keys() and str(key) in merged_dict.keys():\n",
    "            merged_dict[int(word_map[key])] = merged_dict[str(key)]\n",
    "            del merged_dict[str(key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_uncompressed_index(merged_dict):\n",
    "    file_dict = {}\n",
    "    for key in tqdm(merged_dict, position=0, leave=True, desc='saving uncompressed index'):\n",
    "        file_dict[key] = []\n",
    "        for item in merged_dict[key]:\n",
    "            for i in item:\n",
    "                if isinstance(i, int):\n",
    "                    file_dict[key].append(i)\n",
    "                else:\n",
    "                    for a in i:\n",
    "                        file_dict[key].append(a)\n",
    "    f = open('./merge_test.txt', 'w', encoding=\"ISO-8859-1\")\n",
    "    f.write(json.dumps(file_dict, separators=(',', ':')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n\nChanging key name:   0%|          | 0/188734 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nChanging key name: 100%|██████████| 188734/188734 [00:00<00:00, 1117231.63it/s]\n"
    }
   ],
   "source": [
    "change_key_name(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Changing key name:  84%|████████▎ | 157887/188734 [00:20<00:00, 525139.69it/s]\nsaving uncompressed index:  81%|████████  | 153153/188734 [00:14<00:00, 112700.77it/s]\nsaving uncompressed index: 100%|██████████| 188734/188734 [00:14<00:00, 13020.25it/s]\n"
    }
   ],
   "source": [
    "save_uncompressed_index(merged_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def term_freq(word, docno):\n",
    "    return next(item[1] for item in data[word] if item[0] == docno)\n",
    "\n",
    "def doc_freq(word):\n",
    "    if data.get(word) is not None:\n",
    "        return len(data[word])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def doc_length(docno):\n",
    "    return len(stemmed_dict[docno])\n",
    "\n",
    "# def doc_length(docno):\n",
    "#     return len(unstemmed_dict[docno])\n",
    "\n",
    "def tf_q(queryno, word):\n",
    "    count = Counter(query_dict[queryno])\n",
    "    return count[word]\n",
    "\n",
    "def avg_length():\n",
    "    total_len = 0\n",
    "    for docno in docs_dict.keys():\n",
    "        total_len += doc_length(docno)\n",
    "    return total_len / total_docs\n",
    "\n",
    "def doc_filter(word):\n",
    "    if data.get(word) is not None:\n",
    "        return [n[0] for n in data[word]]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "total_docs = len(docs_dict)\n",
    "vocab_size = len(unq_dict)\n",
    "avg_len = avg_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "term frequency 2\ndocument frequency 288\ndocument length 581\naverage length 261.25758756701856\ntotal # documents 84678\nvocabulary size 188734\n"
    }
   ],
   "source": [
    "print('term frequency', term_freq('new', 'AP890101-0001'))\n",
    "print('document frequency', doc_freq('transcript'))\n",
    "print('document length', doc_length('AP890101-0001'))\n",
    "print('average length', avg_len)\n",
    "print('total # documents', total_docs)\n",
    "print('vocabulary size', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Retrieve Queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries for stemmed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'85': ['document', 'discuss', 'alleg', 'measur', 'taken', 'corrupt', 'public', 'offici', 'ani', 'government', 'jurisdict', 'worldwid'], '59': ['document', 'report', 'type', 'weather', 'event', 'ha', 'directli', 'caus', 'least', 'fatal', 'locat'], '56': ['document', 'includ', 'predict', 'prime', 'lend', 'rate', 'report', 'actual', 'prime', 'rate', 'move'], '71': ['document', 'report', 'incurs', 'land', 'air', 'water', 'border', 'area', 'countri', 'militari', 'forc', 'second', 'countri', 'guerrilla', 'group', 'base', 'second', 'countri'], '64': ['document', 'report', 'event', 'result', 'polit', 'motiv', 'hostage', 'tak'], '62': ['document', 'report', 'militari', 'coup', \"d'etat\", 'attempt', 'success', 'ani', 'countri'], '93': ['document', 'describ', 'identifi', 'support', 'nation', 'rifl', 'associ', 'nra', 'asset'], '99': ['document', 'identifi', 'develop', 'iran', 'contra', 'affair'], '58': ['document', 'predict', 'anticip', 'rail', 'strike', 'report', 'ongo', 'rail', 'strike'], '77': ['document', 'report', 'poach', 'method', 'type', 'wildlif'], '54': ['document', 'cite', 'sign', 'contract', 'preliminari', 'agreement', 'make', 'tent', 'reserv', 'launch', 'commerci', 'satellit'], '87': ['document', 'report', 'current', 'crimin', 'action', 'offic', 'fail', 'u.s.', 'financi', 'institut'], '94': ['document', 'identifi', 'crime', 'perpetr', 'aid', 'comput'], '100': ['document', 'identifi', 'effort', 'non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog', 'undesir', 'nation'], '89': ['document', 'identifi', 'exist', 'pend', 'invest', 'opec', 'member', 'state', 'ani', 'downstream', 'oper'], '61': ['document', 'discuss', 'role', 'israel', 'iran', 'contra', 'affair'], '95': ['document', 'describ', 'comput', 'applic', 'crime', 'solv'], '68': ['document', 'report', 'actual', 'studi', 'unsubstanti', 'concern', 'safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul', 'product'], '57': ['document', 'discuss', 'mci', 'ha', 'sinc', 'bell', 'system', 'breakup'], '97': ['document', 'identifi', 'instanc', 'fiber', 'optic', 'technolog', 'actual'], '98': ['document', 'identifi', 'individu', 'organ', 'produc', 'fiber', 'optic', 'equip'], '60': ['document', 'describ', 'side', 'controversi', 'standard', 'perform', 'determin', 'salari', 'level', 'incent', 'pay', 'contrast', 'determin', 'pay', 'sole', 'basi', 'senior', 'longev', 'job'], '80': ['document', 'identifi', 'someth', 'platform', '1988', 'presidenti', 'candid'], '63': ['document', 'identifi', 'machin', 'translat', 'system'], '91': ['document', 'identifi', 'acquisit', 'u.s.', 'armi', 'specifi', 'advanc', 'weapon', 'system']}\n"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "querylist = open('../AP_DATA/query_desc.51-100.short.txt')\n",
    "query_dict = {}\n",
    "word_arr = []\n",
    "query_arr = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for line in querylist:\n",
    "    if line.strip() != '':\n",
    "        queryno = re.sub('[^A-Za-z0-9]+', '', line.split()[0])\n",
    "        query = line.split()[1:]\n",
    "        modified = line[line.find(\".\")+1:]\n",
    "        tokens = word_tokenize(modified)\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        stemmed_tokens = [element for item in stemmed_tokens for element in item.split('-')]\n",
    "        for i in stemmed_tokens:\n",
    "            if i not in stop_arr and re.search('[a-zA-Z0-9]', str(i)):\n",
    "                word_arr.append(i)\n",
    "        query_dict[queryno] = word_arr\n",
    "        word_arr = []\n",
    "        query_arr = []\n",
    "print(query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries for unstemmed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'85': ['Document', 'discuss', 'allegations', 'measures', 'taken', 'corrupt', 'public', 'officials', 'governmental', 'jurisdiction', 'worldwide'], '59': ['Document', 'report', 'type', 'weather', 'event', 'directly', 'caused', 'least', 'fatality', 'location'], '56': ['Document', 'prediction', 'prime', 'lending', 'rate', 'report', 'actual', 'prime', 'rate', 'move'], '71': ['Document', 'report', 'incursions', 'land', 'air', 'water', 'border', 'area', 'country', 'military', 'forces', 'second', 'country', 'guerrilla', 'group', 'based', 'second', 'country'], '64': ['Document', 'report', 'event', 'result', 'politically', 'motivated', 'hostage-taking'], '62': ['Document', 'report', 'military', 'coup', \"d'etat\", 'attempted', 'successful', 'country'], '93': ['Document', 'describe', 'identify', 'supporters', 'National', 'Rifle', 'Association', 'NRA', 'assets'], '99': ['Document', 'identify', 'development', 'Iran-Contra', 'Affair'], '58': ['Document', 'predict', 'anticipate', 'rail', 'strike', 'report', 'ongoing', 'rail', 'strike'], '77': ['Document', 'report', 'poaching', 'method', 'type', 'wildlife'], '54': ['Document', 'cite', 'signing', 'contract', 'preliminary', 'agreement', 'making', 'tentative', 'reservation', 'launch', 'commercial', 'satellite'], '87': ['Document', 'report', 'current', 'criminal', 'actions', 'officers', 'failed', 'U.S.', 'financial', 'institution'], '94': ['Document', 'identify', 'crime', 'perpetrated', 'aid', 'computer'], '100': ['Document', 'identify', 'efforts', 'non-communist', 'industrialized', 'states', 'regulate', 'transfer', 'high-tech', 'goods', 'dual-use', 'technologies', 'undesirable', 'nations'], '89': ['Document', 'identify', 'existing', 'pending', 'investment', 'OPEC', 'member', 'state', 'downstream', 'operation'], '61': ['Document', 'discuss', 'role', 'Israel', 'Iran-Contra', 'Affair'], '95': ['Document', 'describe', 'computer', 'application', 'crime', 'solving'], '68': ['Document', 'report', 'actual', 'studies', 'unsubstantiated', 'concerns', 'safety', 'manufacturing', 'employees', 'installation', 'workers', 'fine-diameter', 'fibers', 'insulation', 'products'], '57': ['Document', 'discuss', 'MCI', 'Bell', 'System', 'breakup'], '97': ['Document', 'identify', 'instances', 'fiber', 'optics', 'technology', 'actually'], '98': ['Document', 'identify', 'individuals', 'organizations', 'produce', 'fiber', 'optics', 'equipment'], '60': ['Document', 'describe', 'sides', 'controversy', 'standards', 'performance', 'determine', 'salary', 'levels', 'incentive', 'pay', 'contrasted', 'determining', 'pay', 'solely', 'basis', 'seniority', 'longevity', 'job'], '80': ['Document', 'identify', 'platform', '1988', 'presidential', 'candidate'], '63': ['Document', 'identify', 'machine', 'translation', 'system'], '91': ['Document', 'identify', 'acquisition', 'U.S.', 'Army', 'specified', 'advanced', 'weapons', 'systems']}\n"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "querylist = open('../AP_DATA/query_desc.51-100.short.txt')\n",
    "query_dict = {}\n",
    "word_arr = []\n",
    "query_arr = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# stemmer = SnowballStemmer('english')\n",
    "stop_arr.append('document')\n",
    "# stop_arr.append('discuss')\n",
    "\n",
    "for line in querylist:\n",
    "    if line.strip() != '':\n",
    "        queryno = re.sub('[^A-Za-z0-9]+', '', line.split()[0])\n",
    "        query = line.split()[1:]\n",
    "        modified = line[line.find(\".\")+1:]\n",
    "        tokens = word_tokenize(modified)\n",
    "        for i in tokens:\n",
    "            if i not in stop_arr and re.search('[a-zA-Z0-9]', str(i)):\n",
    "                word_arr.append(i)\n",
    "        query_dict[queryno] = word_arr\n",
    "        word_arr = []\n",
    "        query_arr = []\n",
    "print(query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score(docno, model_scores, score):\n",
    "    if docno in model_scores:\n",
    "        model_scores[docno] += score\n",
    "    else:\n",
    "        model_scores[docno] = score\n",
    "\n",
    "def rank_scores(key, scores, out):\n",
    "    if len(scores) < 1000:\n",
    "        iter = len(scores)\n",
    "    else:\n",
    "        iter = 1000\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for j in range(iter):\n",
    "        str = ('{} Q0 {} {} {} Exp'\n",
    "            .format(key, sorted_scores[j][0], j+1, sorted_scores[j][1]))\n",
    "        out.write(str+\"\\n\")\n",
    "    scores.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models & computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_tf(tf, doc_len):\n",
    "    return tf / (tf + 0.5 + 1.5 * (doc_len/avg_len))\n",
    "\n",
    "def tf_idf(tf, doc_len, df, total_docs):\n",
    "    return okapi_tf(tf, doc_len) * math.log(total_docs / df)\n",
    "\n",
    "def okapi_bm25(tf, tf_q, doc_len, df, k1, k2, b, total_docs, avg_len):\n",
    "    return math.log((total_docs + 0.5) / (df + 0.5)) * \\\n",
    "           ((tf + k1 * tf) / (tf + k1* ((1-b) + b * (doc_len / avg_len)))) * \\\n",
    "           ((tf_q + k2 * tf_q) / (tf_q + k2))\n",
    "\n",
    "def unigram_lm_laplace(tf, doc_len, voc_size):\n",
    "    return math.log((tf + 1) / (doc_len + voc_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_okapi_tf(query):\n",
    "    okapi_tf_out = open('./okapi-tf.txt', \"a\")\n",
    "    okapi_tf_scores = defaultdict(lambda: 0.0)\n",
    "    for key, words in tqdm(query.items(), position=0, leave=True, desc='Computing Okapi-TF'):\n",
    "        for word in words:\n",
    "            docs = doc_filter(word)\n",
    "            if docs is not None:\n",
    "                for docno in docs:\n",
    "                    tf = term_freq(word, docno)\n",
    "                    doc_len = doc_length(docno)\n",
    "                    add_score(docno, okapi_tf_scores, okapi_tf(tf, doc_len))\n",
    "        rank_scores(key, okapi_tf_scores, okapi_tf_out)\n",
    "    okapi_tf_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_tf_idf(query):\n",
    "    tf_idf_out = open('./tf-idf.txt', \"a\")\n",
    "    tf_idf_scores = defaultdict(lambda: 0.0)\n",
    "    for key, words in tqdm(query.items(), position=0, leave=True, desc='Computing TF-IDF'):\n",
    "        for word in words:\n",
    "            docs = doc_filter(word)\n",
    "            df = doc_freq(word)\n",
    "            if docs is not None:\n",
    "                for docno in docs:\n",
    "                    tf = term_freq(word, docno)\n",
    "                    doc_len = doc_length(docno)\n",
    "                    add_score(docno, tf_idf_scores, tf_idf(tf, doc_len, df, total_docs))\n",
    "        rank_scores(key, tf_idf_scores, tf_idf_out)\n",
    "    tf_idf_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def compute_okapi_bm25(query):\n",
    "    okapi_bm25_out = open('./okapi-bm25.txt', \"a\")\n",
    "    okapi_bm25_scores = defaultdict(lambda: 0.0)\n",
    "    for key, words in tqdm(query.items(), position=0, leave=True, desc='Computing Okapi-BM25'):\n",
    "        for word in words:\n",
    "            docs = doc_filter(word)\n",
    "            if docs is not None:\n",
    "                for docno in docs:\n",
    "                    df = len(docs)\n",
    "                    tf = term_freq(word, docno)\n",
    "                    doc_len = doc_length(docno)\n",
    "                    qf = tf_q(key, word)\n",
    "                    add_score(docno, okapi_bm25_scores, okapi_bm25(tf, qf, doc_len, df, 1.2, 100, 0.75, total_docs, avg_len))\n",
    "        rank_scores(key, okapi_bm25_scores, okapi_bm25_out)\n",
    "    okapi_bm25_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unigram_lm_laplace(query):\n",
    "    laplace_scores = defaultdict(lambda: 0.0)\n",
    "    out = open('./unigram-lm-laplace.txt', \"a\")\n",
    "    for key,words in tqdm(query.items(), position=0, leave=True, desc='Computing Language Models(Unigram-LM-Laplace)'):\n",
    "        for word in words:\n",
    "            docs=doc_filter(word)\n",
    "            for docno in docs_dict.keys():\n",
    "                if docs is not None:\n",
    "                    if docno in docs:\n",
    "                        tf = term_freq(word, docno)\n",
    "                        doc_len = doc_length(docno)\n",
    "                    else:\n",
    "                        tf = 0\n",
    "                        doc_len = 0\n",
    "                score = unigram_lm_laplace(tf, doc_len, vocab_size)\n",
    "                add_score(docno, laplace_scores, score)\n",
    "        rank_scores(key, laplace_scores, out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VSM (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Computing TF-IDF: 100%|██████████| 25/25 [02:45<00:00,  6.61s/it]\n"
    }
   ],
   "source": [
    "compute_tf_idf(query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Okapi-BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Computing Okapi-BM25: 100%|██████████| 25/25 [02:54<00:00,  6.98s/it]\n"
    }
   ],
   "source": [
    "compute_okapi_bm25(query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_unigram_lm_laplace(query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Proximity Search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(blurb):\n",
    "    return max(blurb) - min(blurb)\n",
    "\n",
    "def compute_min_range(possible_combos):\n",
    "    blurb_list = []\n",
    "    for blurb in possible_combos:\n",
    "        blurb_list.append(diff(blurb))\n",
    "    return min(blurb_list)\n",
    "\n",
    "def rank_and_retrieve(key, scores):\n",
    "    top_k_scores = {}\n",
    "    if len(scores) < 1000:\n",
    "        iter = len(scores)\n",
    "    else:\n",
    "        iter = 1000\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for j in range(iter):\n",
    "        # str = ('{} Q0 {} {} {} Exp'\n",
    "        #     .format(key, sorted_scores[j][0], j+1, sorted_scores[j][1]))\n",
    "        # print(str+\"\\n\")\n",
    "        top_k_scores[sorted_scores[j][0]] = sorted_scores[j][1]\n",
    "    return top_k_scores\n",
    "\n",
    "def proximity_search(min_range, num_terms, docno):\n",
    "    return (1500 - min_range) * num_terms / (doc_length(docno) + vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 25/25 [02:12<00:00,  5.31s/it]\n"
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "prox_scores = defaultdict(lambda: 0.0)\n",
    "out = open('./proximity-search.txt', \"a\")\n",
    "\n",
    "countt = 0\n",
    "keys = [*query_dict]\n",
    "\n",
    "for qryno in tqdm(query_dict.keys(), position=0, leave=True):\n",
    "    for docno in stemmed_dict.keys():\n",
    "        num_terms = 0\n",
    "        q_dict = {}\n",
    "        w_list = [n[0] for n in stemmed_dict[docno]]\n",
    "        for q in query_dict[qryno]:\n",
    "            if q in w_list:\n",
    "                num_terms += 1\n",
    "        for q in query_dict[qryno]:\n",
    "            if q in w_list and num_terms > 0:\n",
    "                q_dict[q] = [n[3] for n in stemmed_dict[docno] if n[0] == q]\n",
    "        if q_dict != {}:\n",
    "            possible_combos = [values for values in q_dict.values()]\n",
    "            possible_combos = list(itertools.product(*possible_combos))\n",
    "            min_range = compute_min_range(possible_combos)\n",
    "            add_score(docno, prox_scores, proximity_search(min_range, num_terms, docno))\n",
    "    rank_scores(keys[countt], prox_scores, out)\n",
    "    countt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Proxmity Search using Feedback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Computing Proximity Search: 100%|██████████| 25/25 [15:13<00:00, 36.55s/it]\n"
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "    \n",
    "def compute_proximity_search(query):\n",
    "    countt = 0\n",
    "    keys = [*query_dict]\n",
    "    proximity_out = open('./proximity-search.txt', \"a\")\n",
    "    tf_idf_scores = defaultdict(lambda: 0.0)\n",
    "    prox_scores = defaultdict(lambda: 0.0)\n",
    "    for key, words in tqdm(query.items(), position=0, leave=True, desc='Computing Proximity Search'):\n",
    "        for word in words:\n",
    "            docs = doc_filter(word)\n",
    "            if docs is not None:\n",
    "                for docno in docs:\n",
    "                    df = len(docs)\n",
    "                    tf = term_freq(word, docno)\n",
    "                    doc_len = doc_length(docno)\n",
    "                    qf = tf_q(key, word)\n",
    "                    add_score(docno, tf_idf_scores, tf_idf(tf, doc_len, df, total_docs))\n",
    "        top_k_scores = rank_and_retrieve(key, tf_idf_scores)\n",
    "        for docno in top_k_scores.keys():\n",
    "            num_terms = 0\n",
    "            q_dict = {}\n",
    "            w_list = [n[0] for n in stemmed_dict[docno]]\n",
    "            for q in query_dict[key]:\n",
    "                if q in w_list:\n",
    "                    num_terms += 1\n",
    "            for q in query_dict[key]:\n",
    "                if q in w_list and num_terms > 0:\n",
    "                    q_dict[q] = [n[3] for n in stemmed_dict[docno] if n[0] == q]\n",
    "            if q_dict != {}:\n",
    "                possible_combos = [values for values in q_dict.values()]\n",
    "                possible_combos = list(itertools.product(*possible_combos))\n",
    "                min_range = compute_min_range(possible_combos)\n",
    "                if min_range != 0:\n",
    "                    add_score(docno, prox_scores, proximity_search(min_range, num_terms, docno))\n",
    "        for docno in top_k_scores.keys():\n",
    "            top_k_scores[docno] += prox_scores[docno]\n",
    "        rank_scores(keys[countt], top_k_scores, proximity_out)\n",
    "        prox_scores.clear()\n",
    "        top_k_scores.clear()\n",
    "        tf_idf_scores.clear()\n",
    "        countt += 1\n",
    "\n",
    "compute_proximity_search(query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Computing Proximity Search: 100%|██████████| 25/25 [03:00<00:00,  7.24s/it]\n"
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_proximity_search_unstemmed(query):\n",
    "    countt = 0\n",
    "    keys = [*query_dict]\n",
    "    proximity_out = open('./proximity-search-unstemmed.txt', \"a\")\n",
    "    tf_idf_scores = defaultdict(lambda: 0.0)\n",
    "    prox_scores = defaultdict(lambda: 0.0)\n",
    "    for key, words in tqdm(query.items(), position=0, leave=True, desc='Computing Proximity Search'):\n",
    "        for word in words:\n",
    "            docs = doc_filter(word)\n",
    "            if docs is not None:\n",
    "                for docno in docs:\n",
    "                    df = len(docs)\n",
    "                    tf = term_freq(word, docno)\n",
    "                    doc_len = doc_length(docno)\n",
    "                    qf = tf_q(key, word)\n",
    "                    add_score(docno, tf_idf_scores, tf_idf(tf, doc_len, df, total_docs))\n",
    "        top_k_scores = rank_and_retrieve(key, tf_idf_scores)\n",
    "        for docno in top_k_scores.keys():\n",
    "            num_terms = 0\n",
    "            q_dict = {}\n",
    "            w_list = [n[0] for n in unstemmed_dict[docno]]\n",
    "            for q in query_dict[key]:\n",
    "                if q in w_list:\n",
    "                    num_terms += 1\n",
    "            for q in query_dict[key]:\n",
    "                if q in w_list and num_terms > 0:\n",
    "                    q_dict[q] = [n[3] for n in unstemmed_dict[docno] if n[0] == q]\n",
    "            if q_dict != {}:\n",
    "                possible_combos = [values for values in q_dict.values()]\n",
    "                possible_combos = list(itertools.product(*possible_combos))\n",
    "                min_range = compute_min_range(possible_combos)\n",
    "                if min_range != 0:\n",
    "                    add_score(docno, prox_scores, proximity_search(min_range, num_terms, docno))\n",
    "        for docno in top_k_scores.keys():\n",
    "            top_k_scores[docno] += prox_scores[docno]\n",
    "        rank_scores(keys[countt], top_k_scores, proximity_out)\n",
    "        prox_scores.clear()\n",
    "        top_k_scores.clear()\n",
    "        tf_idf_scores.clear()\n",
    "        countt += 1\n",
    "\n",
    "compute_proximity_search_unstemmed(query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}