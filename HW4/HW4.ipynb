{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAGE RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAGE RANK - other graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def normalize(orig_scores):\n",
    "    scores = orig_scores.copy()\n",
    "    norm = math.sqrt(sum(scores[id] ** 2 for id in scores))\n",
    "    for key in scores:\n",
    "        scores[key] = scores[key] / norm\n",
    "    return scores\n",
    "\n",
    "def perplexity(scores):\n",
    "    list = [scores[url] for url in scores]\n",
    "    return 2 ** entropy(list)\n",
    "\n",
    "def converged(prev_perplexity, curr_perplexity, THRESHOLD):\n",
    "    print('diff:', abs(prev_perplexity - curr_perplexity))\n",
    "    return THRESHOLD > abs(prev_perplexity - curr_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def create_links_map(filepath):\n",
    "    pages = {}\n",
    "    outlinks_dict = defaultdict(set)\n",
    "    inlinks_dict = defaultdict(set)\n",
    "    counter = itertools.count()\n",
    "    with open(filepath) as f:\n",
    "        lines = f.readlines()\n",
    "        # first round\n",
    "        # give each node an integer index in the matrix\n",
    "        for line in lines:\n",
    "            line = line.split()\n",
    "            link, inlinks = line[0], line[1:]\n",
    "            # node_lst.append(link)\n",
    "            pages[link] = next(counter)\n",
    "            for inlink in inlinks:\n",
    "                outlinks_dict[inlink].add(link)\n",
    "                inlinks_dict[link].add(inlink)\n",
    "    return pages, inlinks_dict, outlinks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(pages, inlinks_dict, outlinks_dict):\n",
    "    counter = 1\n",
    "    PR, newPR = {}, {}\n",
    "    N = len(pages)\n",
    "    d = 0.85\n",
    "    S = set([p for p in pages if len(outlinks_dict[p]) == 0])\n",
    "    prev_perplexity = 0\n",
    "    print('pages #', len(pages))\n",
    "\n",
    "    for p in pages:\n",
    "        PR[p] = 1 / N\n",
    "    curr_perplexity = perplexity(normalize(PR))\n",
    "\n",
    "    while True:\n",
    "        print(counter, end = ' ')\n",
    "        if converged(prev_perplexity, curr_perplexity, 1) and counter == 4:\n",
    "            break\n",
    "        prev_perplexity = curr_perplexity\n",
    "        sinkPR = 0\n",
    "        for p in S:\n",
    "            sinkPR += PR[p]\n",
    "        for p in pages:\n",
    "            newPR[p] = ((1 - d) + d * sinkPR ) / N\n",
    "            # newPR[p] += (1 - d) * sinkPR / N\n",
    "            for q in inlinks_dict[p]:\n",
    "                newPR[p] += d * PR[q] / len(outlinks_dict[q])\n",
    "        for p in pages:\n",
    "            PR[p] = newPR[p]\n",
    "        curr_perplexity = perplexity(normalize(PR))\n",
    "        if counter == 4:\n",
    "            counter = 0\n",
    "        counter += 1\n",
    "\n",
    "    sorted_scores = sorted(PR.items(), key=lambda x: x[1], reverse=True)\n",
    "    with open('PR_result.txt', 'w', encoding='ISO-8859-1') as f:\n",
    "        for i in range(500):\n",
    "            url, score = sorted_scores[i]\n",
    "            str = ('{}\\t{}\\t{}\\t{}\\n'.format(url, score, len(outlinks_dict[url]), len(inlinks_dict[url])))\n",
    "            f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pages # 183811\n1 diff: 4456.400980452248\n2 diff: 1952.548406450212\n3 diff: 146.9480189658284\n4 diff: 306.8492682892929\n1 diff: 67.71992391887352\n2 diff: 140.329908728364\n3 diff: 53.09501347359674\n4 diff: 78.45465809264078\n1 diff: 37.151472583815575\n2 diff: 46.0965923866288\n3 diff: 25.364389824873342\n4 diff: 29.393211244821487\n1 diff: 18.434266864159326\n2 diff: 20.033159596367568\n3 diff: 13.167166146306045\n4 diff: 13.369175852930766\n1 diff: 9.081345896047878\n2 diff: 9.097310212334378\n3 diff: 6.5020736102833325\n4 diff: 6.371237501768519\n1 diff: 4.640024410115984\n2 diff: 4.406531632517272\n3 diff: 3.255156429100225\n4 diff: 3.06021549892057\n1 diff: 2.3157302141316904\n2 diff: 2.157843062645952\n3 diff: 1.6533643415218648\n4 diff: 1.5155053646549277\n1 diff: 1.1702250627363355\n2 diff: 1.0636003539311787\n3 diff: 0.8321802654963903\n4 diff: 0.7525964107044274\n"
    }
   ],
   "source": [
    "pages, inlinks_dict, outlinks_dict = create_links_map('wt2g_inlinks.txt')\n",
    "page_rank(pages, inlinks_dict, outlinks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAGE RANK - crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "host='https://elastic:KTPRMHa5BA5fjrh8zwqqIZOQ@0bb7e3d7643c42ae85d3d8ad417593f3.us-east-1.aws.found.io:9243'\n",
    "es = Elasticsearch([host])\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def normalize(orig_scores):\n",
    "    scores = orig_scores.copy()\n",
    "    norm = math.sqrt(sum(scores[id] ** 2 for id in scores))\n",
    "    for key in scores:\n",
    "        scores[key] = scores[key] / norm\n",
    "    return scores\n",
    "\n",
    "def perplexity(scores):\n",
    "    list = [scores[url] for url in scores]\n",
    "    return 2 ** entropy(list)\n",
    "\n",
    "def converged(prev_perplexity, curr_perplexity, THRESHOLD):\n",
    "    print('diff:', abs(prev_perplexity - curr_perplexity))\n",
    "    return THRESHOLD > abs(prev_perplexity - curr_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def es_ids(index):\n",
    "#     a=helpers.scan(es,query={\"query\":{\"match_all\": {}}},scroll='1m',index=index)\n",
    "#     return [aa['_id'] for aa in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pages = es_ids('merged_index121')\n",
    "# with open('pages.data', 'wb') as filehandle:\n",
    "#     pickle.dump(pages, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"pages.data\",\"rb\")\n",
    "pages = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_links_map_es(index, pages):\n",
    "#     outlinks_dict = defaultdict(set)\n",
    "#     inlinks_dict = defaultdict(set)\n",
    "#     for id in tqdm(pages):\n",
    "#         outlinks_dict[id] = get_outlinks(index, id)\n",
    "#         inlinks_dict[id] = get_inlinks(index, id)\n",
    "#     return outlinks_dict, inlinks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inlinks_dict, outlinks_dict = create_links_map_es('merged_index121', pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"inlinks_dict.txt\", 'w') as file:\n",
    "#     json.dump(inlinks_dict, file)\n",
    "#     print('dumped inlinks')\n",
    "\n",
    "# with open(\"outlinks_dict.txt\", 'w') as file:\n",
    "#     json.dump(outlinks_dict, file)\n",
    "#     print('dumped outlinks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"inlinks_dict.txt\") as file:\n",
    "    inlinks_dict = json.loads(file.read())\n",
    "\n",
    "with open(\"outlinks_dict.txt\") as file:\n",
    "    outlinks_dict = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_4_crawl(pages, inlinks_dict, outlinks_dict):\n",
    "    counter = 1\n",
    "    PR, newPR = {}, {}\n",
    "    N = len(pages)\n",
    "    d = 0.85\n",
    "    S = set([p for p in pages if len(outlinks_dict[p]) == 0])\n",
    "    prev_perplexity = 0\n",
    "\n",
    "    for p in pages:\n",
    "        PR[p] = 1 / N\n",
    "    curr_perplexity = perplexity(normalize(PR))\n",
    "\n",
    "    while True:\n",
    "        print(counter, end = ' ')\n",
    "        if converged(prev_perplexity, curr_perplexity, 0.1) and counter == 4:\n",
    "            break\n",
    "        prev_perplexity = curr_perplexity\n",
    "        sinkPR = 0\n",
    "        for p in S:\n",
    "            sinkPR += PR[p]\n",
    "        for p in pages:\n",
    "            newPR[p] = ((1 - d) + d * sinkPR ) / N\n",
    "            # newPR[p] += (1 - d) * sinkPR / N\n",
    "            for q in inlinks_dict[p]:\n",
    "                if q in outlinks_dict.keys() and len(outlinks_dict[q]) != 0:\n",
    "                    # print(len(outlinks_dict[q]), q)\n",
    "                    newPR[p] += d * PR[q] / len(outlinks_dict[q])\n",
    "        for p in pages:\n",
    "            PR[p] = newPR[p]\n",
    "        curr_perplexity = perplexity(normalize(PR))\n",
    "        if counter == 4:\n",
    "            counter = 0\n",
    "        counter += 1\n",
    "\n",
    "    sorted_scores = sorted(PR.items(), key=lambda x: x[1], reverse=True)\n",
    "    with open('PR_crawl_result.txt', 'w', encoding='ISO-8859-1') as f:\n",
    "        for i in range(500):\n",
    "            url, score = sorted_scores[i]\n",
    "            str = ('{}\\t{}\\t{}\\t{}\\n'.format(url, score, len(outlinks_dict[url]), len(inlinks_dict[url])))\n",
    "            f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 diff: 2483.2049682698466\n"
    }
   ],
   "source": [
    "page_rank_4_crawl(pages, outlinks_dict, inlinks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "host='https://elastic:KTPRMHa5BA5fjrh8zwqqIZOQ@0bb7e3d7643c42ae85d3d8ad417593f3.us-east-1.aws.found.io:9243'\n",
    "es = Elasticsearch([host])\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def normalize(orig_scores):\n",
    "    scores = orig_scores.copy()\n",
    "    norm = math.sqrt(sum(scores[id] ** 2 for id in scores))\n",
    "    for key in scores:\n",
    "        scores[key] = scores[key] / norm\n",
    "    return scores\n",
    "\n",
    "def perplexity(scores):\n",
    "    list = [scores[url] for url in scores]\n",
    "    return 2 ** entropy(list)\n",
    "\n",
    "def converged(prev_perplexity, curr_perplexity, THRESHOLD):\n",
    "    print('diff:', abs(prev_perplexity - curr_perplexity))\n",
    "    return THRESHOLD > abs(prev_perplexity - curr_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "query_dict = {}\n",
    "\n",
    "def get_query(query):\n",
    "    global query_dict\n",
    "    stop_arr = []\n",
    "    query_arr = []\n",
    "    word_arr = []\n",
    "    stoplist = open('../HW1/reference/stoplist.txt')\n",
    "\n",
    "    ## STOPLIST\n",
    "    for line in stoplist:\n",
    "        stop_arr.append(line.strip())\n",
    "\n",
    "    ## STEMMER\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    ## QUERY\n",
    "    # for line in query.split():\n",
    "    #     print(line)\n",
    "    #     if line.strip() != '':\n",
    "    #         queryno = re.sub('[^A-Za-z0-9]+', '', line.split()[0])\n",
    "    #         query = line.split()[1:]\n",
    "    #         modified = line[line.find(\".\")+1:]\n",
    "    tokens = word_tokenize(query)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_tokens = [element for item in stemmed_tokens for element in item.split('-')]\n",
    "    for i in stemmed_tokens:\n",
    "        if i not in stop_arr and re.search('[a-zA-Z0-9]', str(i)):\n",
    "            query_arr.append(i)\n",
    "    return query_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score(docno, model_scores, score):\n",
    "    if docno in model_scores:\n",
    "        model_scores[docno] += score\n",
    "    else:\n",
    "        model_scores[docno] = score\n",
    "\n",
    "def create_root(scores, out):\n",
    "    global root\n",
    "    iter = 1000\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for j in range(iter):\n",
    "        root.append(sorted_scores[j][0])\n",
    "        str = ('{}\\t{}\\t{}'\n",
    "            .format(sorted_scores[j][0], j+1, sorted_scores[j][1]))\n",
    "        out.write(str+\"\\n\")\n",
    "    scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "root = []\n",
    "\n",
    "def compute_es_built_in(query, index):\n",
    "    out = open('es-built-in.txt', \"a\")\n",
    "    es_built_in_scores = defaultdict(lambda: 0.0)\n",
    "    for word in query:\n",
    "        print(word)\n",
    "        res=es.search(index=index,body={\"query\":{\"match\":{\"text\":word}},\"size\":5000})\n",
    "        for docno in res['hits']['hits']:\n",
    "            add_score(docno['_id'], es_built_in_scores, docno['_score'])\n",
    "    create_root(es_built_in_scores, out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "list\nmodern\nartist\nlength of root 1000\n"
    }
   ],
   "source": [
    "query = get_query('List of Modern Artists')\n",
    "compute_es_built_in(query, 'merged_index121')\n",
    "print('length of root', len(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inlinks(index, id):\n",
    "    try:\n",
    "        res = es.get(index=index, id=id)['_source']\n",
    "        return res['inlinks']\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_outlinks(index, id):\n",
    "    try:\n",
    "        res = es.get(index=index, id=id)['_source']\n",
    "        return res['outlinks']\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "D = 50\n",
    "\n",
    "def create_base(index):\n",
    "    base = set(root.copy())\n",
    "    # GET INLINKS AND OUTLINKS\n",
    "    for item in root:\n",
    "        inlinks = get_inlinks(index, item)\n",
    "        outlinks = get_outlinks(index, item)\n",
    "        if len(inlinks) >= D:\n",
    "            base.update(random.sample(inlinks, D))\n",
    "        else:\n",
    "            base.update([link for link in inlinks])\n",
    "        if len(outlinks) >= D:\n",
    "            base.update(random.sample(outlinks, D))\n",
    "        else:\n",
    "            base.update([link for link in outlinks])\n",
    "        if len(base) > 10000:\n",
    "            break\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "base length 10040\n"
    }
   ],
   "source": [
    "base = create_base('merged_index121')\n",
    "print('base length', len(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# inlinks_dict = {}\n",
    "# outlinks_dict = {}\n",
    "\n",
    "# def dump_links_map():\n",
    "#     inlinks_path = 'base_inlinks.txt'\n",
    "#     outlinks_path = 'base_outlinks.txt'\n",
    "\n",
    "#     for id in tqdm(base):\n",
    "#         inlinks_dict[id] = get_inlinks('merged_index121', id)\n",
    "#         outlinks_dict[id] = get_outlinks('merged_index121', id)\n",
    "    \n",
    "#     f_inlinks = open(inlinks_path, 'w', encoding=\"ISO-8859-1\")\n",
    "#     f_outlinks = open(outlinks_path, 'w', encoding=\"ISO-8859-1\")\n",
    "#     f_inlinks.write(json.dumps(inlinks_dict))\n",
    "#     f_outlinks.write(json.dumps(outlinks_dict))\n",
    "#     f_inlinks.close()\n",
    "#     f_outlinks.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# dump_links_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"inlinks_dict.txt\") as file:\n",
    "    inlinks_dict = json.loads(file.read())\n",
    "\n",
    "with open(\"outlinks_dict.txt\") as file:\n",
    "    outlinks_dict = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "auth perplexity at iter 1 319.05810660837074 and diff is 319.05810660837074\nhub perplexity at iter 1 245.03028695424635 and diff is 245.03028695424635\nauth perplexity at iter 2 275.33966201624645 and diff is 43.718444592124285\nhub perplexity at iter 2 200.7801081088463 and diff is 44.25017884540006\nauth perplexity at iter 3 243.47298865945183 and diff is 31.86667335679462\nhub perplexity at iter 3 178.04216522026366 and diff is 22.737942888582637\nauth perplexity at iter 4 226.67725754606153 and diff is 16.795731113390303\nhub perplexity at iter 4 166.77415417212487 and diff is 11.268011048138789\nauth perplexity at iter 5 218.3912295827866 and diff is 8.286027963274933\nhub perplexity at iter 5 161.24428939597618 and diff is 5.529864776148685\nauth perplexity at iter 6 214.34701873089793 and diff is 4.044210851888664\nhub perplexity at iter 6 158.50560929080925 and diff is 2.738680105166935\nauth perplexity at iter 7 212.35828728083874 and diff is 1.9887314500591913\nhub perplexity at iter 7 157.1316085611908 and diff is 1.3740007296184444\nauth perplexity at iter 8 211.36828245759816 and diff is 0.9900048232405823\nhub perplexity at iter 8 156.43381613633647 and diff is 0.6977924248543275\nauth perplexity at iter 9 210.8693543858757 and diff is 0.498928071722446\nhub perplexity at iter 9 156.0756650484018 and diff is 0.35815108793468653\nauth perplexity at iter 10 210.61511439432667 and diff is 0.2542399915490421\nhub perplexity at iter 10 155.89015006573257 and diff is 0.1855149826692184\nauth perplexity at iter 11 210.48431217102433 and diff is 0.13080222330233937\nhub perplexity at iter 11 155.79329641336503 and diff is 0.09685365236754251\nauth perplexity at iter 12 210.41646516193165 and diff is 0.06784700909267372\nhub perplexity at iter 12 155.74238888291168 and diff is 0.05090753045334395\n"
    }
   ],
   "source": [
    "hub = {id: 1.0 for id in base}\n",
    "auth = {id: 1.0 for id in base}\n",
    "\n",
    "MAX_ITER = 100\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "def compute_HITS():\n",
    "    counter = 1\n",
    "    prev_auth = 0\n",
    "    prev_hub = 0\n",
    "    i = 0\n",
    "    for i in range(MAX_ITER):\n",
    "        global hub, auth\n",
    "        hub_list, auth_list = [], []\n",
    "\n",
    "        for id in auth:\n",
    "            if id in inlinks_dict.keys():\n",
    "                # for link in get_inlinks('merged_index12', id):\n",
    "                for link in inlinks_dict[id]:\n",
    "                    try:\n",
    "                        hub_list.append(hub[link])\n",
    "                    except:\n",
    "                        pass\n",
    "            auth[id] = sum(hub_list)\n",
    "            hub_list.clear()\n",
    "        auth = normalize(auth)\n",
    "        curr_auth = perplexity(auth)\n",
    "        auth_diff = abs(curr_auth - prev_auth)\n",
    "        print('auth perplexity at iter', i+1, curr_auth, 'and diff is', auth_diff)\n",
    "        prev_auth = curr_auth\n",
    "\n",
    "        for id in hub:\n",
    "            if id in inlinks_dict.keys():\n",
    "                # for link in get_outlinks('merged_index12', id):\n",
    "                for link in outlinks_dict[id]:\n",
    "                    try:\n",
    "                        auth_list.append(auth[link])\n",
    "                    except:\n",
    "                        pass\n",
    "            hub[id] = sum(auth_list)\n",
    "            auth_list.clear()\n",
    "        hub = normalize(hub)\n",
    "        curr_hub = perplexity(hub)\n",
    "        hub_diff = abs(curr_hub - prev_hub)\n",
    "        print('hub perplexity at iter', i+1, curr_hub, 'and diff is', hub_diff)\n",
    "        prev_hub = curr_hub\n",
    "\n",
    "        if (hub_diff < THRESHOLD or auth_diff < THRESHOLD) and counter == 4:\n",
    "            break\n",
    "        if counter == 4:\n",
    "            counter = 0\n",
    "        counter += 1\n",
    "\n",
    "compute_HITS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_top_500(filename, dictionary):\n",
    "    sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "    with open(filename, 'w', encoding='ISO-8859-1') as f:\n",
    "        for i in range(500):\n",
    "            url, score = sorted_dict[i]\n",
    "            str = ('{}\\t{}\\n'.format(url, score))\n",
    "            f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_top_500('auth_top_500.txt', auth)\n",
    "write_top_500('hub_top_500.txt', hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}